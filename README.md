# container-hardway


<summary>Traditional Software Deployment - Dependency Hell</summary>
<details>
# üß© Traditional Software Deployment ‚Äî The ‚ÄúDependency Hell‚Äù

Before containers and modern DevOps, deploying software was a **nightmare**.  
Let‚Äôs explore how **dependency hell**, **library conflicts**, and **environment drift** created chaos across teams ‚Äî from application developers to infrastructure operators.

---

## ‚öôÔ∏è 1. Overview

Traditional software deployment meant installing applications **directly on the host machine** (bare-metal or VM).  
Everything ‚Äî libraries, configurations, binaries ‚Äî lived together in the **same OS environment**.

While this worked for small systems, it quickly fell apart as complexity grew.

---

## üí£ 2. The ‚ÄúDependency Hell‚Äù

**Dependency Hell** refers to the pain of managing software dependencies across environments.

### üî∏ Example Scenario
You build an app requiring:
- Python 3.8  
- `numpy==1.21.0`  
- `pandas==1.3.0`

But the production server already runs:
- Python 3.10  
- `numpy==1.18.0` (used by another app)

Result:  
Your app **fails** due to version mismatches ‚Äî and updating breaks other applications.

### üî∏ Causes
- Shared system libraries between multiple apps  
- Conflicting package versions  
- OS-level dependency differences (Ubuntu vs CentOS)  
- Manual installs & lack of isolation  
- No reproducible environment (works on my machine üòÖ)

---

## üß± 3. Application Layer Problems

| Problem | Description |
|----------|--------------|
| üß© Library Conflicts | Different apps require different versions of the same library. |
| üß† Environment Drift | Dev, staging, and production environments slowly become inconsistent. |
| üîÑ Manual Deployment | Developers manually copy code, run scripts, install libs ‚Äî error-prone and inconsistent. |
| üßç Human Dependency | Deployment success often depends on a specific person who ‚Äúknows the setup.‚Äù |

---

## üì¶ 4. Library & System Dependency Problems

| Issue | Impact |
|-------|---------|
| Global package installs | Breaks other apps sharing the same host. |
| Different OS versions | Missing or incompatible system libs. |
| Manual package management | Difficult to track what‚Äôs installed where. |
| No rollback mechanism | Once broken, hard to recover previous working state. |

---

## üß∞ 5. Operations (Ops) Problems

| Problem | Description |
|----------|-------------|
| üîß Configuration Drift | Configs change manually over time ‚Äî no single source of truth. |
| ‚ö†Ô∏è Hard to Reproduce | Rebuilding a server requires manual steps and tribal knowledge. |
| ‚è≥ Slow Provisioning | Installing dependencies, setting up databases, etc., takes hours/days. |
| üß® No Isolation | One app crash or update may affect all apps on the same host. |

---

## üèóÔ∏è 6. Infrastructure Problems

| Problem | Description |
|----------|-------------|
| üß± Static Infrastructure | Servers are manually configured and long-lived. |
| üö´ No Scalability | Hard to scale apps quickly ‚Äî provisioning new servers is slow. |
| üß© Inconsistent Environments | Every server might be slightly different. |
| üíÄ Hard Recovery | When a server dies, rebuilding is time-consuming. |

---

## üî• 7. The Result ‚Äî Chaos Everywhere

- ‚Äú**Works on my machine**‚Äù syndrome  
- Downtime during deployments  
- Conflicts between dev, ops, and infra teams  
- No automation or repeatability  
- Complex rollback and debugging  
- Tight coupling between app and environment  

---

## üåà 8. The Turning Point

To solve these problems came **modern solutions**:
- **Virtual Machines (VMs)** ‚Üí First level of isolation  
- **Containers (Docker, Kubernetes)** ‚Üí Lightweight, portable environments  
- **Infrastructure as Code (IaC)** ‚Üí Automate provisioning (Terraform, Ansible)  
- **CI/CD Pipelines** ‚Üí Automate build, test, and deployment  

---

## ‚úÖ 9. Summary

| Layer | Traditional Problem | Modern Fix |
|--------|----------------------|-------------|
| Application | Dependency conflicts | Containers / Virtual Environments |
| Libraries | Version mismatch | Dependency managers (pip, npm, etc.) |
| Operations | Manual config & deploy | CI/CD pipelines |
| Infrastructure | Static servers | IaC (Terraform, Ansible, CloudFormation) |

---

## üß≠ 10. Key Takeaway

> Traditional deployment bound **applications, dependencies, and infrastructure tightly together** ‚Äî making systems fragile, slow to change, and painful to maintain.  
>  
> Modern DevOps and containerization **decouple these layers**, bringing consistency, scalability, and reproducibility.

---


</details>


<summary>Virtual Machine</summary>
<details>

Each VM behaves like a **separate computer**, isolated from others, even though they share the same physical hardware.

---

```
+-----------------------------------+
| Physical Hardware (CPU, RAM, Disk)|
+-----------------------------------+
| Host OS (e.g., Ubuntu, Windows) |
+-----------------------------------+
| Hypervisor (VMware, VirtualBox) |
+-----------------------------------+
| Guest OS 1 | Guest OS 2 | Guest OS 3 |
| (Ubuntu) | (CentOS) | (Windows) |
+-----------------------------------+
| App A | App B | App C |
```

## üß© 2. Key Components

| Component | Description |
|------------|--------------|
| **Host OS** | The operating system installed on the physical machine. |
| **Guest OS** | The OS running inside the VM (can be different from the host). |
| **Hypervisor** | The layer that manages VMs, allocating CPU, memory, and disk resources. |
| **Virtual Hardware** | Virtual CPU, memory, disk, and network interfaces that mimic real hardware. |

---

## üß± 3. Types of Hypervisors

| Type | Description | Examples |
|------|--------------|-----------|
| **Type 1 (Bare-metal)** | Runs directly on hardware, no host OS. Best performance. | VMware ESXi, Microsoft Hyper-V, Xen |
| **Type 2 (Hosted)** | Runs on top of a host OS. Easier to use but slower. | VirtualBox, VMware Workstation |

---

## üß∞ 4. How VMs Solve Traditional Problems

| Traditional Problem | How VMs Help |
|----------------------|--------------|
| üß© Dependency Conflicts | Each VM has its own OS and libraries ‚Äî no interference. |
| üß† Environment Drift | VM images can be cloned, ensuring consistent environments. |
| ‚öôÔ∏è Ops Overhead | Automate provisioning using VM templates. |
| üíÄ Server Failure | Snapshots and backups allow quick recovery. |
| üîí Security | Isolation prevents one app from crashing another. |

---

## ‚ö° 5. VM Workflow Example

1. **Create VM image** with OS + dependencies (e.g., Ubuntu + Python 3.8).  
2. **Deploy app** into the VM.  
3. **Snapshot or clone** the VM for staging/production.  
4. **Run multiple VMs** on one server, each isolated.  
5. **Monitor and manage** using hypervisor tools.

---

## üß± 6. Benefits of Virtual Machines

| Advantage | Description |
|------------|--------------|
| ‚úÖ **Isolation** | Each VM runs independently, preventing conflicts. |
| üîÅ **Reproducibility** | VM images can be replicated exactly. |
| üîí **Security** | Compromise in one VM doesn't affect others. |
| ‚öôÔ∏è **Flexibility** | Can run different OS types (Linux, Windows, etc.) on one host. |
| üíæ **Snapshots** | Easy backup and rollback. |
| ‚òÅÔ∏è **Cloud Adoption** | Enabled cloud computing (AWS EC2, Azure VMs). |

---

## üß® 7. Limitations of Virtual Machines

| Limitation | Description |
|-------------|--------------|
| üê¢ **Heavyweight** | Each VM runs a full OS ‚Äî consumes large memory & disk. |
| ‚ö° **Slow Startup** | Booting a VM takes minutes (vs. seconds for containers). |
| üíΩ **Resource Duplication** | Multiple OS instances duplicate system resources. |
| üîÑ **Complex Management** | Requires hypervisor-level setup and patching. |
| üß© **Limited Portability** | Moving VMs between platforms can be slow and large (GBs). |

---

## üèóÔ∏è 8. Virtual Machines vs. Traditional Deployment

| Feature | Traditional Deployment | Virtual Machines |
|----------|------------------------|------------------|
| Environment Isolation | ‚ùå Shared OS | ‚úÖ Full OS per app |
| Dependency Conflicts | ‚ùå Frequent | ‚úÖ Eliminated |
| Portability | ‚ùå Low | ‚öôÔ∏è Moderate |
| Resource Usage | ‚ö° Light | üê¢ Heavy |
| Startup Speed | ‚ö° Fast | üê¢ Slow |
| Rollback | ‚ùå Hard | ‚úÖ Easy (snapshots) |

---

## üåç 9. VMs in Modern Infrastructure

Even today, VMs remain **the foundation of the cloud**:
- **AWS EC2**, **Azure Virtual Machines**, **Google Compute Engine** ‚Üí all are VM-based.  
- Containers (like Docker) often run **on top of VMs** for extra isolation.

> üí° VMs provide hardware-level isolation,  
> while containers provide process-level isolation ‚Äî lightweight but less isolated.

---

## ‚úÖ 10. Summary

| Concept | Key Idea |
|----------|-----------|
| Virtual Machine | A full OS running inside another OS. |
| Hypervisor | The software that manages VMs. |
| Benefit | Isolation, reproducibility, security. |
| Downside | Heavy, slower, and resource-hungry. |
| Modern Role | Foundation for cloud platforms and container hosts. |

---

## üß≠ 11. Key Takeaway

> Virtual Machines were the **first major solution** to dependency hell and environment inconsistency.  
> They brought **isolation, portability, and disaster recovery**,  
> but at the cost of **performance and efficiency** ‚Äî leading to the rise of **containers** (Docker, Kubernetes).

---



</details>

<summary>What is Container</summary>
<details>
# üì¶ Containers ‚Äî Lightweight, Portable, and Fast Deployment

After Virtual Machines (VMs), the next revolution in software deployment was **Containers** ‚Äî  
a lightweight way to package, ship, and run applications **consistently** across environments.

---

## ‚öôÔ∏è 1. What Is a Container?

A **Container** is a **lightweight, isolated environment** that bundles:
- Your **application code**
- All its **dependencies**, **libraries**, and **configurations**

It shares the **same host OS kernel**, unlike a Virtual Machine (which runs a full OS per app).

---

### üß© Simplified View
```
Traditional Deployment:
App A + Libs ---> Host OS
App B + Libs ---> Host OS ‚Üí Conflicts!

Virtual Machines:
App + Libs + Guest OS ---> Hypervisor ---> Host OS
App + Libs + Guest OS ---> Hypervisor ---> Host OS

Containers:
App + Libs ---> Container Runtime ---> Host OS (shared kernel)
App + Libs ---> Container Runtime ---> Host OS
```


Each container runs **isolated**, but they all share the **same OS kernel**, making them **lightweight and fast**.

---

## üß± 2. Core Components

| Component | Description |
|------------|--------------|
| **Container Image** | A read-only template that defines what‚Äôs inside the container (OS libs, app code, env vars). |
| **Container Runtime** | The engine that runs containers (e.g., Docker, containerd). |
| **Dockerfile** | Blueprint for building images (defines base image, commands, dependencies). |
| **Container Registry** | Storage for images (Docker Hub, AWS ECR, GCP Artifact Registry). |

---

## üß∞ 3. How Containers Work

1. **Build** an image from a `Dockerfile`  
   ‚Üí includes code, dependencies, configs.

2. **Run** the image as a container  
   ‚Üí isolated environment with its own filesystem, processes, and network.

3. **Ship** the same image to dev, test, and prod  
   ‚Üí runs identically everywhere.


## Containers vs Virtual Machines
| Feature            | Virtual Machines                   | Containers                   |
| ------------------ | ---------------------------------- | ---------------------------- |
| **Isolation**      | Full OS isolation                  | Process-level isolation      |
| **Startup Time**   | Minutes                            | Seconds                      |
| **Size**           | GBs                                | MBs                          |
| **Performance**    | Heavy (hypervisor overhead)        | Near-native                  |
| **Portability**    | High (image-based)                 | Very high                    |
| **Resource Usage** | Each VM duplicates OS              | Shared kernel ‚Äî efficient    |
| **Use Case**       | Full OS isolation, strong security | Fast, scalable microservices |

## üîí 5. Container Isolation Mechanisms

### Containers use Linux kernel features:

- Namespaces ‚Üí isolate processes (PID, network, mount, user)
- cgroups (Control Groups) ‚Üí limit CPU, memory, I/O usage
- UnionFS ‚Üí layer filesystem for images (fast & space-efficient)

üß† Essentially, containers are isolated processes, not virtual machines.

## Benefits of Containers
| Benefit            | Description                                                  |
| ------------------ | ------------------------------------------------------------ |
| üöÄ **Lightweight** | No need for full OS per app ‚Äî faster and smaller.            |
| üîÅ **Portable**    | Runs anywhere (dev, test, prod, cloud).                      |
| ‚öôÔ∏è **Consistent**  | Same image = same environment across all systems.            |
| üß± **Modular**     | Each service can run in its own container (microservices).   |
| üß© **Scalable**    | Easily scale up/down using orchestration tools (Kubernetes). |
| üîí **Isolated**    | Apps run independently, reducing interference.               |

## Limitations of Containers
| Limitation                   | Description                                                         |
| ---------------------------- | ------------------------------------------------------------------- |
| üîê **Shared Kernel**         | Containers share the same OS kernel ‚Äî less isolation than VMs.      |
| üß± **State Persistence**     | Containers are ephemeral; data must be stored externally (volumes). |
| üß∞ **Security Risks**        | Kernel vulnerabilities can affect all containers.                   |
| ‚öôÔ∏è **Complex Orchestration** | Managing hundreds of containers needs tools like Kubernetes.        |

</details>


# Linux Container - Main Components

## NameSpace
<summary>What is NameSpace</summary>
<details>
# üß© Linux Namespaces ‚Äî The Core of Container Isolation

**Namespaces** are one of the key mechanisms that make **containers** possible.  
They provide **process isolation** ‚Äî giving each container its own view of system resources.

---

## ‚öôÔ∏è 1. What Is a Namespace?

A **namespace** is a **Linux kernel feature** that wraps a set of system resources and presents them to a process as if it were the only process using those resources.

> üß† Think of a namespace as a "private world" for a process ‚Äî  
> it sees only its own processes, network, mounts, and users.

Namespaces allow containers to appear **independent**, even though they share the same kernel.

---

## üîí 2. Why Namespaces Exist

Without namespaces:
- All processes see the same PID list.
- All network interfaces are shared.
- All users belong to the same system.

With namespaces:
- Each process gets its **own PID tree**, **network stack**, and **mount view**.
- Processes are **isolated**, even while running on the same OS.

---

## üß± 3. Types of Namespaces

| Namespace | System Resource Isolated | Example | Description |
|------------|---------------------------|----------|--------------|
| üóÇÔ∏è **Mount (mnt)** | Filesystem mounts | `/proc`, `/etc` | Each container has its own view of the filesystem. |
| üßç **Process ID (pid)** | Process tree | `ps`, `top` | A container sees only its own processes (PID 1 = its init). |
| üß† **UTS (Unix Timesharing System)** | Hostname, domain | `hostname` | Each container can have its own hostname. |
| üë§ **User (user)** | User and group IDs | `root`, `uid/gid` | A process can be root inside a container but unprivileged outside. |
| üåê **Network (net)** | Network stack | `eth0`, `lo`, IPs | Each container gets its own network interfaces and routing tables. |
| ‚öôÔ∏è **CGroup (cgroup)** | Resource management | CPU, memory | Used with cgroups to limit CPU/memory usage per container. |
| üí¨ **IPC (Inter-Process Communication)** | Shared memory, message queues | `shm`, `sem` | Processes communicate only within their own IPC namespace. |
| ‚è∞ **Time (time)** | System clock | `date`, `hwclock` | Each container can have its own view of system time. (newer feature) |

---

## üß© 4. Example: PID Namespace

### üß† Concept
Each container thinks it has its own process tree starting from **PID 1**.

### üîç Without Namespace
```bash
ps aux
# Shows all system processes
```
### üß± With Namespace
```
unshare --pid --fork --mount-proc bash
ps aux

# Shows only processes inside this namespace
```
The process inside sees itself as PID 1 ‚Äî just like an independent OS.

### User Namespace Example

Allows mapping of user IDs inside and outside containers.

Example:
A process can run as root (UID 0) inside the container,
but as a non-root user on the host.

```
unshare --user --map-root-user bash
whoami
# Output: root
```

The process believes it‚Äôs root, but it‚Äôs safely mapped to an unprivileged user outside.

### Working With Namespaces
| Command       | Purpose                                             |
| ------------- | --------------------------------------------------- |
| **`unshare`** | Create a new namespace and run a command inside it. |
| **`nsenter`** | Enter an existing namespace from another process.   |


#### Example 1 ‚Äî Create a new namespace
```
unshare --uts --pid --net --mount bash
```

=> Creates new hostname, process, network, and mount namespaces.

### Example 2 ‚Äî Enter a namespace
```
nsenter --target <pid> --net
```

=> oins the network namespace of another process.

### Combining Namespaces

| Namespace | Purpose                                               |
| --------- | ----------------------------------------------------- |
| `mnt`     | Isolate filesystem mounts                             |
| `pid`     | Isolate process tree                                  |
| `uts`     | Isolate hostname                                      |
| `user`    | Isolate permissions                                   |
| `net`     | Isolate network interfaces                            |
| `ipc`     | Isolate inter-process communication                   |
| `time`    | Isolate clock                                         |
| `cgroup`  | Limit resource usage (works together with namespaces) |

</details>

## Control Group
<summary>What is Cgroup</summary>
<details>
# ‚öôÔ∏è Linux Control Groups (cgroups) ‚Äî Resource Management for Containers

**Control Groups (cgroups)** are a **Linux kernel feature** that allow the system to **limit, isolate, and measure resource usage** (CPU, memory, I/O, etc.) for groups of processes.  
They work **together with namespaces** to make containers truly isolated and efficient.

---

## üß© 1. What Are Control Groups?

> **cgroups = control + groups**

They let you:
- **Allocate** specific hardware resources to a group of processes  
- **Limit** how much CPU, memory, or disk I/O they can use  
- **Measure** their usage and performance  
- **Isolate** them from other groups

> üß† Think of cgroups as ‚Äúresource fences‚Äù around processes ‚Äî  
> ensuring one container cannot starve others.

---

## üß± 2. How Containers Use cgroups

When you run a container (e.g., with Docker or Kubernetes):
- A **new cgroup** is created for that container.
- CPU, memory, and I/O usage are tracked and limited based on configuration.
- The kernel enforces these limits automatically.

Example:  
```bash
docker run --memory=256m --cpus=0.5 nginx
```
=> Docker translates these flags into cgroup settings behind the scenes.

### Types of cgroups (Controllers)
| Controller        | Description                                                              |
| ----------------- | ------------------------------------------------------------------------ |
| üß† **cpuset**     | Assigns specific CPUs and memory nodes to tasks. Useful for CPU pinning. |
| ‚öôÔ∏è **cpu**        | Controls CPU scheduling ‚Äî how much processing time tasks get.            |
| üìä **cpuacct**    | Reports CPU usage for accounting and monitoring.                         |
| üî¢ **pids**       | Limits the number of processes in a group. Prevents fork bombs.          |
| üíæ **io / blkio** | Limits read/write operations to block devices (disk I/O).                |
| üßÆ **memory**     | Sets limits on RAM usage; can trigger OOM killer if exceeded.            |
| üíΩ **devices**    | Controls access to device files (`/dev/sda`, `/dev/null`, etc.).         |
| üßä **freezer**    | Suspends or resumes all tasks in the group.                              |
| üåê **net_cls**    | Tags network packets with class IDs for traffic control.                 |
| üì∂ **net_prio**   | Dynamically sets network interface priorities per group.                 |
| üéØ **perf_event** | Restricts performance event monitoring.                                  |
| üìè **hugetlb**    | Enables huge pages usage for memory-intensive workloads.                 |

### Common Use Cases
| Use Case                     | Example                                     |
| ---------------------------- | ------------------------------------------- |
| Limit memory per container   | `memory.limit_in_bytes = 512M`              |
| Limit CPU time               | `cpu.shares = 512`                          |
| Restrict number of processes | `pids.max = 100`                            |
| Control I/O throughput       | `blkio.throttle.write_bps_device = 1048576` |
| Assign specific CPU cores    | `cpuset.cpus = 0,1`                         |

### Working with cgroups (File System Interface)

cgroups expose a virtual filesystem (usually mounted at `/sys/fs/cgroup`).

```
/sys/fs/cgroup/
‚îú‚îÄ‚îÄ cpu/
‚îÇ   ‚îú‚îÄ‚îÄ cgroup.procs
‚îÇ   ‚îú‚îÄ‚îÄ cpu.shares
‚îÇ   ‚îî‚îÄ‚îÄ cpu.cfs_quota_us
‚îú‚îÄ‚îÄ memory/
‚îÇ   ‚îú‚îÄ‚îÄ memory.limit_in_bytes
‚îÇ   ‚îú‚îÄ‚îÄ memory.usage_in_bytes
‚îÇ   ‚îî‚îÄ‚îÄ cgroup.procs
‚îî‚îÄ‚îÄ pids/
    ‚îú‚îÄ‚îÄ pids.max
    ‚îî‚îÄ‚îÄ cgroup.procs
```

### üß© Example: Create and Limit a Process

#### 1Ô∏è‚É£ Create a new cgroup for memory
```
mkdir /sys/fs/cgroup/memory/testgroup
```

#### 2Ô∏è‚É£ Set a 100MB memory limit
```
echo 104857600 > /sys/fs/cgroup/memory/testgroup/memory.limit_in_bytes
```

#### 3Ô∏è‚É£ Add a process (e.g., PID 12345) to that group
```
echo 12345 > /sys/fs/cgroup/memory/testgroup/cgroup.procs
```
 => ‚úÖ That process can now use up to 100MB RAM only.

### Checking Usage

You can monitor usage directly via cgroup files:
```
cat /sys/fs/cgroup/memory/testgroup/memory.usage_in_bytes
cat /sys/fs/cgroup/cpu/testgroup/cpuacct.usage
```

=> These are updated in real-time by the kernel.

### Freezing and Resuming Processes

You can suspend or resume all processes in a cgroup:

```
# Freeze
echo FROZEN > /sys/fs/cgroup/freezer/testgroup/freezer.state

# Resume
echo THAWED > /sys/fs/cgroup/freezer/testgroup/freezer.state

```

### cgroups + namespaces = containers
| Mechanism      | Purpose                                                    |
| -------------- | ---------------------------------------------------------- |
| **Namespaces** | Isolate what a process can see (filesystem, PID, network). |
| **Cgroups**    | Control what a process can use (CPU, memory, I/O).         |

### Summary
| Concept         | Description                                                                      |
| --------------- | -------------------------------------------------------------------------------- |
| **cgroups**     | Kernel mechanism to control and account system resources per group of processes. |
| **Controllers** | Manage specific resources (CPU, memory, IO, etc.).                               |
| **Interface**   | Exposed via virtual filesystem `/sys/fs/cgroup/`.                                |
| **Integration** | Used by Docker, Kubernetes, and systemd.                                         |
| **Goal**        | Prevent one process or container from consuming all resources.                   |


</details>

## Network basic
<summary>Network Interface</summary>
<details>
# üß© 1Ô∏è‚É£ Network Interfaces ‚Äî Deep & Easy Explanation

---

## üí° What Is a Network Interface

A **network interface** is like a ‚Äúdoor‚Äù your computer uses to talk to the outside world (or to itself).  
Each ‚Äúdoor‚Äù has a **name**, **address**, and **rules** about how data goes through it.

Your system can have:

| Type | Example | Description |
|------|----------|--------------|
| **Physical Interface** | `eth0`, `wlan0` | Actual hardware (Ethernet port, Wi-Fi card). |
| **Virtual Interface** | `lo`, `docker0`, `vethabc123` | Software-created interfaces (for local or container networking). |

---

## üß† Analogy

- Your computer ‚Üí üè¢ **Building**  
- Interface ‚Üí üö™ **Door**  
- IP address ‚Üí üìÆ **Door number**  
- MAC address ‚Üí üî¢ **Door‚Äôs unique serial number**  
- Packets ‚Üí ‚úâÔ∏è **Letters** going in/out of each door  

---

## ‚öôÔ∏è How It Works (Step-by-Step)

### üåç Example: Opening `https://example.com` in Browser

#### 1Ô∏è‚É£ Application ‚Üí Kernel
- Browser sends request:

```
GET / HTTP/1.1
Host: example.com
```

- The kernel‚Äôs **TCP/IP stack** adds layers:
- TCP ‚Üí adds ports, sequence numbers.
- IP ‚Üí adds source & destination IPs.
- Ethernet ‚Üí adds MAC addresses.

---

#### 2Ô∏è‚É£ Kernel Chooses Interface

The **routing table** decides which interface to use.

Example:
```bash
$ ip route
default via 192.168.1.1 dev eth0
192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.100
```

‚û°Ô∏è The kernel sees the destination is not local,
so it sends packets through eth0 (your Ethernet interface).

### 3Ô∏è‚É£ Driver & NIC

- The kernel gives the packet to the driver for eth0.
- The NIC (Network Interface Card) turns it into electrical or radio signals and transmits it to your router/switch.

### üì• Incoming Packets (Receiving)

- When a packet comes in:
- NIC receives signal from the network.
- NIC raises an interrupt ‚Üí ‚ÄúHey, data arrived!‚Äù
- The driver copies it into a kernel buffer (skb).
-The network stack processes it layer by layer:
- Ethernet ‚Üí IP ‚Üí TCP.
- 
The packet is finally delivered to the application socket.

### üì¶ Example: Loopback Interface (lo)
- Interface name: lo
- IP: 127.0.0.1
- Used to communicate with yourself.

When you run:
```
curl http://127.0.0.1:8080
```

Flow:
```
Browser ‚Üí Kernel ‚Üí lo ‚Üí Kernel ‚Üí Local Server
```

=> No physical network used ‚Äî everything stays inside your system.

### üê≥ Example: Docker Virtual Interfaces
When Docker runs, it creates virtual interfaces.

```
$ ip link
3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 ...
4: vethabcd1234@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> ...
```

- docker0 ‚Üí acts like a virtual switch.
- Each container gets a veth pair:
      - One side in container (eth0 inside container)
      - One side in host (vethXYZ)

Flow:
```
Container ‚Üí eth0 (vethA) ‚Üí vethB (host) ‚Üí docker0 bridge ‚Üí eth0 ‚Üí Internet
```
=> This is how containers communicate with each other and the outside network.

### Important Interface Attributes
| Attribute       | Meaning                           | Example             |
| --------------- | --------------------------------- | ------------------- |
| **MAC Address** | Unique hardware ID (Layer 2)      | `00:1a:2b:3c:4d:5e` |
| **IP Address**  | Logical network address (Layer 3) | `192.168.1.100`     |
| **MTU**         | Max packet size (bytes)           | `1500`              |
| **State**       | Interface active/inactive         | `UP`, `DOWN`        |

### Putting It All Together
```
[Browser] 
   ‚Üì
[Socket API]
   ‚Üì
[TCP/IP Stack]
   ‚Üì
[Routing Table ‚Üí Chooses Interface]
   ‚Üì
[eth0 Driver]
   ‚Üì
[NIC ‚Üí Converts to Signal]
   ‚Üì
[Switch/Router ‚Üí Internet]

```

## ‚úÖ Summary
| Concept                | Description                                             |
| ---------------------- | ------------------------------------------------------- |
| **Interface**          | ‚ÄúDoor‚Äù connecting your system to a network.             |
| **Driver**             | Translates between kernel packets and hardware signals. |
| **NIC**                | Physical hardware for sending/receiving.                |
| **Virtual Interfaces** | Software-based interfaces for containers/VPNs.          |
| **Loopback (`lo`)**    | Local-only interface for internal communication.        |

</details>

<summary>Loopback Interface LO</summary>
<details>
## 2Ô∏è‚É£ Loopback Interface (`lo`)

### üß© What It Is
The **loopback interface (`lo`)** is a **virtual network device** that exists purely in software.  
It allows a computer to communicate **with itself** using the **same networking stack** used for real network communication.

- **Device name:** `lo`
- **IP address:** `127.0.0.1`
- **Network:** `127.0.0.0/8` (entire range reserved for loopback)
- **MAC address:** none (no Layer 2 hardware)
- **Scope:** Local (never leaves host)
- **Purpose:** Internal communication, debugging, and service binding.

---

### üß† Why Loopback Exists

Without `lo`, your machine couldn‚Äôt talk to itself using TCP/IP protocols.  
The loopback device provides:
- A **consistent interface** for testing network applications.
- A **local target** for processes that communicate via sockets (e.g., a web server and browser on the same host).
- A way to **use all network layers** (TCP, IP) without actual hardware transmission.

Example:

```
curl http://127.0.0.1:8080
```

Here, your browser (client) and local server (server) use **the same TCP/IP stack**, just routed internally through `lo`.

---

### ‚öôÔ∏è How It Works ‚Äî Packet Flow (Inside Kernel)

#### 1Ô∏è‚É£ Application Layer
- Process A (client) creates a socket and calls `connect("127.0.0.1", 8080)`.
- Process B (server) is already bound to `127.0.0.1:8080` and listening.

#### 2Ô∏è‚É£ Socket & TCP Layer
- TCP builds a SYN packet.
- The packet is given a source IP `127.0.0.1` and destination IP `127.0.0.1`.

#### 3Ô∏è‚É£ Routing Decision
The **routing table** is checked:

```
$ ip route
127.0.0.0/8 dev lo scope link
```

‚Üí The kernel decides that packets to 127.0.0.0/8 should go through `lo`.

#### 4Ô∏è‚É£ Transmission (TX Path)
- The packet enters the **loopback driver** (`drivers/net/loopback.c` in Linux).
- Instead of going to a physical NIC, it is immediately **re-injected** into the receive path (RX) of the same kernel.
- The **skb (socket buffer)** is cloned and queued for the receive handler.

#### 5Ô∏è‚É£ Reception (RX Path)
- The packet is handed back to the kernel‚Äôs IP input function (`ip_input()`).
- It is routed to the socket listening on `127.0.0.1:8080`.
- The receiving process (server) gets the data via `recv()`.

#### 6Ô∏è‚É£ Response
- The reverse happens for the reply packet (server ‚Üí client), all within the same memory space.

---

### üß© Key Insight: No Hardware, No Interrupts
- No DMA (Direct Memory Access)
- No PHY/MAC layer
- No real transmission queue
- Everything happens in **software** inside the kernel.

So packets move like:
```
App ‚Üí Socket ‚Üí TCP ‚Üí IP ‚Üí lo driver ‚Üí IP ‚Üí Socket ‚Üí App
```

No physical wire, no real NIC.

---

### üîç Example with Tools

#### Routing & Interface
```
$ ip addr show lo
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default
inet 127.0.0.1/8 scope host lo
```

#### Packet Capture
```
sudo tcpdump -i lo
```
You will actually see TCP packets (SYN, ACK, DATA), but **they never leave the machine**.

### üß© In Network Namespaces
Each `netns` (container or isolated env) gets its **own `lo` interface**.

By default, it is **down** when a new namespace is created:

</details>

<summary>Network Stack</summary>
<details>
# üåê 3Ô∏è‚É£ Network Stack Layers ‚Äî Deep & Easy Explanation

---

## üß© What Is the Network Stack?

The **network stack** is the set of software layers that handle how data travels  
from your **application** to the **physical network** (and back).

Each layer adds or removes information (called **headers**) to help the next layer  
understand **where** the data should go and **how** it should be handled.

---

## üß± The OSI Model (7 Layers)

| Layer | Name | Example Protocols | Role |
|-------|------|-------------------|------|
| 7 | **Application** | HTTP, FTP, SSH, DNS | What you actually use (apps talk here) |
| 6 | **Presentation** | SSL/TLS, JSON, JPEG | Data formatting & encryption |
| 5 | **Session** | NetBIOS, RPC | Manage sessions/connections |
| 4 | **Transport** | TCP, UDP | Reliable or fast delivery |
| 3 | **Network** | IP, ICMP | Routing between machines |
| 2 | **Data Link** | Ethernet, ARP | Communication between devices on same network |
| 1 | **Physical** | Fiber, Copper, Wi-Fi | Actual electrical/radio signals |

---

## üß© TCP/IP Model (Used in Real OS)

Modern systems simplify the OSI model into **4 layers**:

| TCP/IP Layer | Corresponding OSI Layers | Example Protocols | Description |
|---------------|--------------------------|-------------------|--------------|
| **Application** | 5‚Äì7 | HTTP, DNS, SMTP | User-level communication |
| **Transport** | 4 | TCP, UDP | Ports, reliability, flow control |
| **Internet** | 3 | IP, ICMP | Logical addressing & routing |
| **Link** | 1‚Äì2 | Ethernet, ARP, Wi-Fi | Physical & local network access |

---

## ‚öôÔ∏è How Data Flows (Concrete Example)

Let‚Äôs say you open your browser and visit `https://example.com`.

### üñ•Ô∏è Application Layer
Your browser (client) sends:
```
GET / HTTP/1.1
Host: example.com
```

‚û°Ô∏è Uses **HTTP (port 443)**, **TLS** for encryption.

---

### üö¶ Transport Layer
- Browser opens a **TCP connection** (a ‚Äúvirtual wire‚Äù between two IPs/ports).
- TCP ensures:
  - **Reliable delivery**
  - **Order preservation**
  - **Retransmission** if packets lost

Example:
- Source Port: 52344
- Destination Port: 443
- Flags: SYN, ACK, etc.


If you use UDP (e.g., for DNS or streaming), it just sends ‚Äî **no reliability**.

---

### üåç Internet Layer
- Adds **IP header**:
  - Source IP: your device
  - Destination IP: server‚Äôs IP
- Decides route using **routing table**.
- Uses **ICMP** for error and diagnostic messages (like `ping`).

Example:
```
IP src=192.168.1.100 dst=93.184.216.34
```


---

### üîå Link Layer
Converts IP packet into **Ethernet frame**:
```
[Dst MAC][Src MAC][Type=IPv4][Payload][CRC]
```
- Uses **ARP (Address Resolution Protocol)** to find destination MAC:

Who has 192.168.1.1? Tell 192.168.1.100 
- Sends data to NIC (Network Interface Card), which converts bits into signals.

---

### üì° Physical Layer
- NIC sends electrical (Ethernet) or radio (Wi-Fi) signals.
- Switches/routers forward them until they reach destination.

At the server side, the process **reverses**:
```
Signal ‚Üí Ethernet ‚Üí IP ‚Üí TCP ‚Üí HTTP ‚Üí Web server
```


---

## üì¶ Packet Encapsulation Example

Every layer **wraps** the data with its own header ‚Äî like putting a letter in envelopes:
```
[Ethernet Header]
[IP Header]
[TCP Header]
[HTTP Data]
```


When receiving, each layer **unwraps** its part and passes the rest upward.

---

## üîÅ Full Round Trip (Simplified Flow)

```
App: "GET /"
‚Üì
[TCP: adds ports, sequence numbers]
‚Üì
[IP: adds source & destination IPs]
‚Üì
[Ethernet: adds MAC addresses]
‚Üì
NIC sends ‚Üí Switch ‚Üí Router ‚Üí Internet ‚Üí Server
‚Üì
Server replies (reverse path)
‚Üì
Your browser renders page
```

### Flow inside your OS:
| Layer                  | What Happens                    |
| ---------------------- | ------------------------------- |
| **Application (HTTP)** | `curl` builds HTTP request      |
| **Transport (TCP)**    | Opens socket on port 80         |
| **Internet (IP)**      | Routes to server IP             |
| **Link (Ethernet)**    | Finds next-hop MAC using ARP    |
| **Physical**           | NIC sends bits to switch/router |

```
[HTTP Data]
   ‚Üì
[TCP Header + HTTP Data]
   ‚Üì
[IP Header + TCP + HTTP]
   ‚Üì
[Ethernet Header + IP + TCP + HTTP]
   ‚Üì
--> NIC --> Cable/Wi-Fi --> Network
```

### ‚úÖ Key Takeaways

- Each layer adds a header to guide the packet.
- Each layer only understands its own header.
- When receiving, the system unwraps each layer in reverse.
- OS implements this ‚Äústack‚Äù inside the kernel network subsystem.
- Tools like tcpdump, wireshark, or /proc/net/ help visualize this stack in action.

</details>

<summary>Routing Table</summary>
<details>
# üß≠ 4Ô∏è‚É£ Routing Table ‚Äî Deep & Easy Explanation

---

## üí° What Is a Routing Table?

A **routing table** is like a **map** your operating system uses to decide **where to send network packets**.

When your system needs to send a packet, it checks this table to figure out:

- Which **network interface** to use (`eth0`, `wlan0`, etc.)
- Which **next hop (gateway)** to forward it to
- Whether the destination is **local** (same subnet) or **remote**

---

## üß† Analogy

Think of it like sending a letter üì¨:

| Scenario | What Happens |
|-----------|---------------|
| Same neighborhood | You can deliver it directly. |
| Different city | You give it to the post office (gateway). |
| Unknown destination | You have no route ‚Äî it‚Äôs dropped. |

---

## ‚öôÔ∏è Routing Table Structure

Each routing entry (row) usually includes:

| Column | Meaning | Example |
|---------|----------|----------|
| **Destination** | Network/subnet the route applies to | `192.168.1.0/24` |
| **Gateway (Next Hop)** | Where to send packets next | `192.168.1.1` |
| **Genmask / Prefix** | Subnet mask | `255.255.255.0` or `/24` |
| **Interface (Iface)** | Which NIC to send through | `eth0`, `wlan0` |
| **Metric** | Priority (lower = preferred) | `100` |

---

## üîç View Your Routing Table

```bash
# Modern Linux command
ip route

# Traditional command
route -n
```

```
default via 192.168.1.1 dev eth0
192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.100
172.17.0.0/16 dev docker0 scope link
```

### üß© Understanding Each Route
| Route                              | Meaning                                                                   |
| ---------------------------------- | ------------------------------------------------------------------------- |
| `default via 192.168.1.1 dev eth0` | ‚ÄúSend everything I don‚Äôt recognize to 192.168.1.1 (the router) via eth0.‚Äù |
| `192.168.1.0/24 dev eth0`          | ‚ÄúI can directly reach all hosts in 192.168.1.x network.‚Äù                  |
| `172.17.0.0/16 dev docker0`        | ‚ÄúPackets for Docker containers go through the docker0 virtual bridge.‚Äù    |


## üîÑ How Routing Decision Works (Step-by-Step)

Let‚Äôs say your machine (IP: 192.168.1.100) sends a packet to 8.8.8.8.

#### 1Ô∏è‚É£ Kernel Looks Up Routing Table
```
1Ô∏è‚É£ Kernel Looks Up Routing Table
```
=> It checks all routes and matches the longest prefix (most specific route).

#### 2Ô∏è‚É£ Route Found: Default Gateway
No direct route to 8.8.8.8, so it matches:
```
default via 192.168.1.1 dev eth0
```

‚Üí Send via router 192.168.1.1 using interface eth0.

#### 3Ô∏è‚É£ Next Hop MAC (ARP Resolution)

The system must find the MAC address of 192.168.1.1 using ARP:
```
Who has 192.168.1.1? Tell 192.168.1.100
```

#### 4Ô∏è‚É£ Packet Encapsulation

The IP packet to 8.8.8.8 is wrapped in an Ethernet frame:
```
Dst MAC = router's MAC
Src MAC = your NIC
IP src = 192.168.1.100
IP dst = 8.8.8.8
```

#### 5Ô∏è‚É£ NIC Sends the Packet
- Your NIC transmits the frame.
- The router then forwards it to the next network (based on its routing table).

### üß† Routing Table Matching Logic
When deciding which route to use:

- Find all routes that match the destination IP.
- Choose the one with the longest prefix (most specific match).
- If tied, pick the one with lowest metric.
- If still tied, the OS may choose based on route insertion order.

```
Destination     Prefix    Interface
192.168.1.0     /24       eth0
192.168.1.128   /25       eth1
```
=> ‚Üí For 192.168.1.130, /25 (eth1) is chosen because it‚Äôs more specific.

### üß© Local vs Remote Routes
| Type                | Description             | Example                         |
| ------------------- | ----------------------- | ------------------------------- |
| **Local route**     | For same subnet         | `192.168.1.0/24`                |
| **Default route**   | For everything else     | `default via 192.168.1.1`       |
| **Host route**      | For a single IP         | `10.10.10.5/32 via 192.168.1.1` |
| **Multicast route** | For group communication | `224.0.0.0/4`                   |

### üß± Static vs Dynamic Routing

| Type        | Description                                | Example                 |
| ----------- | ------------------------------------------ | ----------------------- |
| **Static**  | Manually configured (`ip route add`)       | Home computers, servers |
| **Dynamic** | Automatically updated by routing protocols | Routers, large networks |

Dynamic routing protocols:

- RIP ‚Äî simple, old
- OSPF ‚Äî link-state routing for LANs
- BGP ‚Äî used between ISPs on the Internet

### üß∞ Commands
| Action                         | Command Example                                  |
| ------------------------------ | ------------------------------------------------ |
| Show routing table             | `ip route`                                       |
| Add new route                  | `sudo ip route add 10.10.0.0/16 via 192.168.1.1` |
| Delete route                   | `sudo ip route del 10.10.0.0/16`                 |
| Show routes for specific table | `ip route show table main`                       |
| View default gateway           | `ip route show default`                          |

### üì¶ Example Scenario'

#### üßÆ Machine Configuration
```
IP: 192.168.1.100/24
Gateway: 192.168.1.1
Interface: eth0
```

#### Routing Table
```
default via 192.168.1.1 dev eth0
192.168.1.0/24 dev eth0 scope link src 192.168.1.100
```

#### Destination Cases
| Destination  | Match          | Route           | Interface |
| ------------ | -------------- | --------------- | --------- |
| 192.168.1.55 | 192.168.1.0/24 | Direct          | eth0      |
| 10.0.0.2     | default        | via 192.168.1.1 | eth0      |


### üß© Visualization

```
[Application] ‚Üí [TCP/IP Stack]
          ‚Üì
  [Routing Table Lookup]
          ‚Üì
  ‚îú‚îÄ‚îÄ Local Network ‚Üí Send Directly (ARP)
  ‚îî‚îÄ‚îÄ Remote Network ‚Üí Forward to Gateway
          ‚Üì
       [eth0] ‚Üí [Router] ‚Üí [Internet]
```

### ‚úÖ Summary
| Concept                  | Description                                |
| ------------------------ | ------------------------------------------ |
| **Routing Table**        | OS ‚Äúmap‚Äù to decide where to send packets   |
| **Default Gateway**      | Router used when no specific route matches |
| **Local Route**          | Directly reachable subnet                  |
| **Next Hop**             | The next device that will forward packets  |
| **Metric**               | Priority among multiple routes             |
| **Longest Prefix Match** | Most specific route wins                   |

</details>

<summary>Iptables / Nftables</summary>
<details>
# üî• iptables / nftables ‚Äî How the Kernel Filters and Modifies Packets

## üß© What They Are

Both **iptables** and **nftables** are frameworks inside the **Linux kernel networking stack** that control **how packets are filtered, modified, or forwarded**.

- **iptables** ‚Üí legacy system (used for decades)
- **nftables** ‚Üí modern replacement (faster, unified, cleaner syntax)

They allow administrators to:
- **Allow / block packets** (firewall)
- **Perform NAT** (Network Address Translation)
- **Log or modify packets**
- **Redirect traffic**
- **Rate limit or mangle packets**

---

## ‚öôÔ∏è How It Works ‚Äî Inside the Kernel

When a packet enters or leaves your machine, it **passes through several "hooks" in the kernel networking stack**.

These hooks are controlled by **Netfilter**, the subsystem used by both iptables and nftables.

### üß± Netfilter Hooks

| Hook Point | Direction | Description |
|-------------|------------|-------------|
| **PREROUTING** | Incoming | Before routing decision (good for DNAT) |
| **INPUT** | Incoming | Packet destined **for the local machine** |
| **FORWARD** | Transit | Packet **being routed through** the machine |
| **OUTPUT** | Outgoing | Locally generated packets |
| **POSTROUTING** | Outgoing | After routing (good for SNAT, masquerade) |

---

## üîÑ Packet Flow Example

Let‚Äôs say your computer acts as a router:
```
[eth0] ---> [PREROUTING] ---> [FORWARD] ---> [POSTROUTING] ---> [eth1]
```

Or if it‚Äôs receiving packets for itself:
```
[eth0] ---> [PREROUTING] ---> [INPUT] ---> [Local Process]
```

Or if it‚Äôs sending packets:
```
[Local Process] ---> [OUTPUT] ---> [POSTROUTING] ---> [eth0]
```

### üß© nftables ‚Äî The Modern Way
`nftables` replaces all iptables tables with one unified rule engine.

#### Key Improvements:
- One consistent syntax (no separate filter, mangle, etc.).
- Rules compiled to efficient bytecode executed by kernel.
- Atomic updates (no packet loss when changing rules).
- Smaller and faster.

```
nft add table inet myfilter
nft add chain inet myfilter input { type filter hook input priority 0; }
nft add rule inet myfilter input tcp dport 22 ip saddr != 10.0.0.5 drop
```

### üöÄ Example Flow in Real Life
Imagine:

- You‚Äôre running a web server on 192.168.1.10
- You use iptables to allow port 80 and drop everything else.

```
iptables -A INPUT -p tcp --dport 80 -j ACCEPT
iptables -A INPUT -j DROP
```

When a client sends GET /index.html:

- Packet arrives on NIC ‚Üí kernel triggers PREROUTING hook.
- Routing table sees it‚Äôs for 192.168.1.10 ‚Üí goes to INPUT chain.
- iptables checks: port 80? yes ‚Üí ACCEPT.
- Kernel passes packet up to nginx.
- nginx responds ‚Üí packet passes through OUTPUT ‚Üí POSTROUTING ‚Üí NIC sends it out.

### üß© Connection Tracking (conntrack)
| State           | Description                                 |
| --------------- | ------------------------------------------- |
| **NEW**         | New connection request (e.g., SYN)          |
| **ESTABLISHED** | Ongoing connection                          |
| **RELATED**     | Related to an existing one (e.g., FTP data) |
| **INVALID**     | Broken / malformed packet                   |

```
iptables -A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT
```

=> Allows replies for existing connections, while blocking random unsolicited packets.

### üß© NAT Example ‚Äî Masquerade
When your Linux host connects private clients to the internet:
```
iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE
```

It means:

- Outgoing packets from LAN ‚Üí POSTROUTING.
- Source IP replaced with your public IP (SNAT).
- Replies are automatically rewritten back.

### üß† Summary
| Concept                     | Description                                   |
| --------------------------- | --------------------------------------------- |
| **Netfilter**               | Kernel framework for packet handling          |
| **iptables / nftables**     | User tools that configure Netfilter           |
| **Tables / Chains / Rules** | Define how packets are filtered or rewritten  |
| **Hooks**                   | Integration points in kernel packet path      |
| **conntrack**               | Tracks connection state for smarter filtering |
| **NAT**                     | Rewrites packet source/destination IPs        |

```
[ NIC ] ‚Üí PREROUTING ‚Üí (routing) ‚Üí INPUT or FORWARD ‚Üí POSTROUTING ‚Üí [ NIC ]
                   ‚Üë                    ‚Üì
             (iptables/nftables rules decide fate)

```
</details>

<summary>üåê Network Namespaces (`netns`)</summary>
<details>
# üåê Network Namespaces (`netns`)

## üß© What It Is

A **network namespace** is an **isolated copy of the Linux networking stack**.

Each namespace has its own:
- **Network interfaces** (`eth0`, `lo`, `vethXXX`)
- **Routing table**
- **iptables/nftables rules**
- **/proc/net/** entries
- **ARP cache**
- **Socket connections**

> üß† In short: each `netns` behaves like a separate ‚Äúmini networking world‚Äù ‚Äî with its own IPs, routes, and firewall rules.

---

## ‚öôÔ∏è Why It Exists

Linux namespaces isolate resources between processes ‚Äî `netns` isolates **network resources**.

Used in:
- **Containers (Docker, Kubernetes)**  
- **VPNs / Virtual networking**
- **Testing / network simulation**
- **Per-application isolation**

---

## üß± Analogy

Imagine your computer‚Äôs network stack as a **room full of cables**.

A network namespace is like **building another room** with its **own independent cables**, even though both rooms exist on the same machine.

---

## üß∞ Example ‚Äî Creating and Inspecting a `netns`

### üß† Step 1: Create a new namespace
```bash
ip netns add blue
```

=> Now you have a new isolated network world called blue.

### üß† Step 2: List all namespaces
```
ip netns list
# blue
```

### üß† Step 3: Check interfaces inside it
```
ip netns exec blue ip link
```

Output:
```
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default
```

=> üß© Only the loopback interface (lo) exists by default ‚Äî meaning this namespace can only talk to itself.

## üß© Connecting Namespaces Together (via veth pairs)
`A veth pair acts like a virtual cable ‚Äî two interfaces connected back-to-back.`

```
ip link add veth-blue type veth peer name veth-host
ip link set veth-blue netns blue
ip addr add 10.0.0.1/24 dev veth-host
ip netns exec blue ip addr add 10.0.0.2/24 dev veth-blue
ip link set veth-host up
ip netns exec blue ip link set veth-blue up
```

Now:

- `veth-host` is in the root namespace (your main system)
- `veth-blue` is inside the blue namespace

They can ping each other:
```
ip netns exec blue ping 10.0.0.1
```

You‚Äôve just created an isolated network environment connected to your main system ‚Äî exactly how Docker containers connect to your host.

## üß† Each netns Has Its Own Routing Table
Inside `blue`:
```
ip netns exec blue ip route
```
Output:
```
10.0.0.0/24 dev veth-blue proto kernel scope link src 10.0.0.2
```

### üß† Behind the Scenes ‚Äî File Descriptors
Each namespace is represented in `/var/run/netns/` or `/proc/[pid]/ns/net.`


### üß≠ Summary
| Concept                          | Description                         |
| -------------------------------- | ----------------------------------- |
| **Network Namespace**            | Isolated instance of network stack  |
| **veth pair**                    | Virtual cable between namespaces    |
| **Routing Table, ARP, iptables** | Unique per namespace                |
| **Use cases**                    | Containers, VPNs, network isolation |
| **Tools**                        | `ip netns`, `nsenter`, `ip link`    |

```
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ       Root netns      ‚îÇ
                ‚îÇ  eth0  docker0        ‚îÇ
                ‚îÇ    ‚Üë        ‚Üë         ‚îÇ
                ‚îÇ    ‚îÇ        ‚îÇ         ‚îÇ
                ‚îÇ  veth-host  ‚îÇ         ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
                     ‚îÇ
             (veth pair - virtual cable)
                     ‚îÇ
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ   netns blue ‚îÇ
                ‚îÇ   veth-blue  ‚îÇ
                ‚îÇ   lo         ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
Each namespace = separate networking world.
veth connects them together.
iptables/nftables rules apply per namespace.

</details>


<summary>Virtual Interface</summary>
<details>
# üåê Virtual Network Interfaces

## üß© What They Are

A **virtual interface** is a **software-defined network interface** ‚Äî it behaves like a real NIC (Network Interface Card), but exists **entirely in software**, with no physical hardware.

Linux treats these interfaces just like physical ones (`eth0`, `wlan0`), but they are **created and managed by the kernel or network tools** to serve special purposes (bridging, tunneling, container networking, etc.).

---

## üß± Types of Virtual Interfaces

| Type | Description | Example Use Case |
|------|--------------|------------------|
| **lo (loopback)** | Interface that sends packets to itself | Localhost communication |
| **veth (virtual Ethernet pair)** | Two linked interfaces acting like a virtual cable | Connect containers or namespaces |
| **bridge (br0)** | Software switch that connects multiple interfaces | Docker bridge, VMs |
| **tun/tap** | Virtual point-to-point interface for tunneling | VPNs (OpenVPN, WireGuard) |
| **bond (bond0)** | Combine multiple NICs for redundancy or throughput | High-availability servers |
| **macvlan / ipvlan** | Create multiple virtual interfaces with unique MAC/IPs on one NIC | Container networking |
| **docker0, cni0** | Bridges automatically created by container runtimes | Docker, Kubernetes networking |

---

## üß† Why Virtual Interfaces Exist

They provide **network flexibility without hardware** ‚Äî enabling:
- Containers and VMs to have their own interfaces
- Virtual switches, routers, and tunnels
- Custom routing and traffic control setups
- VPNs and overlay networks
- Testing environments

---

## ‚öôÔ∏è 1Ô∏è‚É£ The Loopback Interface (`lo`)

**Every Linux system** has a loopback interface:

```bash
ip addr show lo
```

Output:
```
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536
    inet 127.0.0.1/8 scope host lo
```

- It‚Äôs always up.
- Used for local communication (e.g., applications talking to each other via localhost).
- Packets sent to 127.0.0.1 never leave your machine ‚Äî they go straight from the application back into the kernel.

### üß© Example:
```
ping 127.0.0.1
```

‚Üí ICMP packets are sent from your system to itself.


### ‚öôÔ∏è 2Ô∏è‚É£ Virtual Ethernet (veth) Pairs
A veth pair works like a patch cable ‚Äî packets entering one side come out the other.
```
ip link add veth-a type veth peer name veth-b
ip link set veth-a up
ip link set veth-b up
```
Now, traffic sent to veth-a appears on veth-b, and vice versa.

Common use: </br>

- One end in the root namespace
- The other in a network namespace (container)

```
ip netns add ns1
ip link set veth-b netns ns1
ip addr add 10.0.0.1/24 dev veth-a
ip netns exec ns1 ip addr add 10.0.0.2/24 dev veth-b
ip link set veth-a up
ip netns exec ns1 ip link set veth-b up
```
Now both sides can ping each other:
```
ip netns exec ns1 ping 10.0.0.1
```


### ‚öôÔ∏è 3Ô∏è‚É£ Bridge Interface (br0)
A bridge acts like a virtual switch ‚Äî it connects multiple interfaces together at Layer 2 (Ethernet).

```
ip link add name br0 type bridge
ip link set br0 up
ip link set veth-a master br0
```

Now, veth-a is connected to the software bridge br0. <br>

Used in: </br>
- Docker: the docker0 bridge connects containers to each other and the host.
- KVM: VMs connect to bridges for LAN-like access.

### üß© Real-World Example ‚Äî Docker Network
When you run:
```
docker run -d nginx
```

Docker creates:

- veth pair connecting container ‚Üí docker0 bridge
- Inside container: eth0 (one end of veth)
- On host: vethXXXX (other end)
- NAT rules via iptables for Internet access


### üß† Visual Overview
```
                     +------------------------+
                     |      Host Network      |
                     |                        |
                     |  eth0   docker0 (br0)  |
                     |    |          ‚Üë        |
                     |    |       veth pair   |
                     +----|----------|--------+
                          |          |
                    [veth-container] [veth-host]
                          ‚Üì
                  +----------------+
                  |   Container     |
                  |   eth0: 172.17.0.2 |
                  +----------------+

```

</details>

<summary>Socket Layer</summary>
<details>
# üß© Socket Layer (Layer Between User Space and Kernel Networking)

## üåê What It Is

The **Socket Layer** is the **bridge between user space (applications)** and the **kernel network stack**.  
It‚Äôs the interface that lets programs send and receive data over a network ‚Äî **without needing to know the low-level details** of Ethernet, IP, or TCP.

Every network program (like `curl`, `nginx`, `ssh`, `ping`) uses **sockets** to talk to the network.

---

## ‚öôÔ∏è How It Works ‚Äî High Level

When your application calls `socket()`, the kernel:
1. **Allocates a socket structure** in kernel space.
2. **Binds it** to a protocol (e.g., TCP, UDP, raw IP).
3. **Connects** it to a local IP:port and possibly a remote IP:port.
4. **Handles all send/receive operations** through the network stack.

User space only sees a **file descriptor (FD)** ‚Äî but the kernel manages everything under the hood.

---

## üß† Analogy

Think of the socket as a **mailbox slot**:
- You put a message (packet) in it (`send()`).
- The post office (kernel) routes it to the correct destination.
- You can receive messages (`recv()`) from it.

---

## üß© Socket Lifecycle ‚Äî Step by Step

Let‚Äôs walk through what happens when an app opens a TCP connection.

### 1Ô∏è‚É£ `socket()`

Creates a socket:
```c
int fd = socket(AF_INET, SOCK_STREAM, 0);
```

- AF_INET ‚Üí IPv4
- SOCK_STREAM ‚Üí TCP (reliable stream)
- Returns a file descriptor (like an open file)
- üß† The kernel allocates a struct socket and struct sock internally.


### 2Ô∏è‚É£ bind()
- Binds the socket to an IP address and port:

```
bind(fd, {ip: 192.168.1.10, port: 8080});
```

Registers:

- Local IP
- Local port
- Protocol type

### 3Ô∏è‚É£ listen() (for servers)

```
listen(fd, 128);
```
The kernel moves it into a listening state, ready to accept connections.

### 4Ô∏è‚É£ connect() (for clients)

```
Initiates a connection to a remote socket:
connect(fd, {ip: 10.0.0.5, port: 80});
```

### 5Ô∏è‚É£ accept() (for servers)
Server accepts incoming connection:
```
int conn_fd = accept(fd, ...);
```

Kernel:

- Creates a new socket for this connection.
- Keeps original socket in listening state.

Now the server can read/write on conn_fd.

### 6Ô∏è‚É£ send() / recv()

- User space: provides data buffer.
- Kernel: copies data from user ‚Üí kernel memory.
- TCP/UDP layers: handle fragmentation, retransmission, checksums.
- NIC driver: sends packets to network card.

Incoming packets:

- NIC triggers interrupt ‚Üí kernel network stack ‚Üí finds socket ‚Üí copies data ‚Üí user buffer (recv() returns).
- üß† Every socket FD has a receive buffer and send buffer managed by the kernel.

### 7Ô∏è‚É£ close()
```
close(fd);
```

### üîÑ Relationship to the Kernel Network Stack
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      User-space App          ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ socket(), send(), recv() ‚îÇ ‚îÇ
‚îî‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÄ‚îò
            ‚îÇ  (syscalls)
            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Socket Layer (Kernel API)  ‚îÇ
‚îÇ - struct socket / sock       ‚îÇ
‚îÇ - protocol dispatch (TCP/UDP)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Protocol Stack (TCP/IP)    ‚îÇ
‚îÇ   Routing / Netfilter / ARP  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Network Driver / NIC       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üîç Example ‚Äî TCP Server Flow
```
Server:
socket() ‚Üí bind() ‚Üí listen() ‚Üí accept() ‚Üí read()/write() ‚Üí close()

Client:
socket() ‚Üí connect() ‚Üí read()/write() ‚Üí close()
```
</details>

# Docker Network
<summary>How to make containers think each of them has a dedicated network stack?</summary>
<details>
  # üß© How Containers Think They Have Their Own Network Stack

When you run a container (e.g., Docker, Podman, Kubernetes pod), it *appears* to have its own independent network interface, routing table, and IP address.  
In reality, this isolation is achieved using **Linux network namespaces (netns)** and **virtual Ethernet (veth) pairs**.

Below, we‚Äôll walk through how this illusion is created ‚Äî step-by-step.

---

## 1Ô∏è‚É£ What Is a Network Namespace?

A **network namespace** is a Linux kernel feature that allows a process (or group of processes) to have its **own isolated view of the network stack**.

Each namespace can have:
- Its own **network interfaces**
- Its own **IP addresses**
- Its own **routing table**
- Its own **iptables/nftables**
- Its own **/proc/net** entries

So, processes inside different namespaces **cannot see or interfere** with each other‚Äôs network.

> üß† In short: each network namespace is like a separate mini-networking world inside the same kernel.

---

## 2Ô∏è‚É£ Step 1 ‚Äî Create a New Network Namespace

```bash
ip netns add netns1
```
üëâ This creates a new namespace called netns1.

At this point, netns1 has:

- No interfaces (except a default loopback lo, which is down).
- No routes.
- No access to the host network.

### 3Ô∏è‚É£ Step 2 ‚Äî Create a Virtual Ethernet (veth) Pair
```
ip link add veth1 type veth peer name ceth1
```

This creates two connected virtual interfaces:
| Interface | Lives in                  | Purpose                                        |
| --------- | ------------------------- | ---------------------------------------------- |
| `veth1`   | Root namespace (host)     | Acts like a ‚Äúcable end‚Äù on the host            |
| `ceth1`   | To be moved into `netns1` | Acts like the ‚Äúother end‚Äù inside the container |

üí° Think of a veth pair as a virtual wire connecting two network stacks.

### 4Ô∏è‚É£ Step 3 ‚Äî Move One End into the New Namespace
```
ip link set ceth1 netns netns1
```

Now:

- veth1 stays in the root namespace.
- ceth1 moves into netns1.

They remain connected, just like two ends of an Ethernet cable.

### 5Ô∏è‚É£ Step 4 ‚Äî Assign IP Addresses and Bring Interfaces Up
#### üîπ On the host (root namespace)

```
ip addr add 172.16.16.201/24 dev veth1
ip link set veth1 up
```
This makes veth1 active on the host with IP 172.16.16.201.
#### üîπ Inside the new namespace (netns1)

```
ip netns exec netns1 ip addr add 172.16.16.101/24 dev ceth1
ip netns exec netns1 ip link set ceth1 up
ip netns exec netns1 ip link set lo up
```


### 6Ô∏è‚É£ Step 5 ‚Äî Test the Connectivity
You can now test communication between the two ends.
```
ping 172.16.16.201 -c 2
```

If run from inside netns1:
```
ip netns exec netns1 ping 172.16.16.201
```

‚úÖ You‚Äôll see a successful ping ‚Äî because ceth1 ‚Üî veth1 are directly connected.

```
     +---------------------+        +--------------------------+
     |  Root Namespace     |        |   netns1 (Container)     |
     |---------------------|        |--------------------------|
     |  veth1              | <----> |  ceth1                   |
     |  IP: 172.16.16.201  |        |  IP: 172.16.16.101       |
     +---------------------+        +--------------------------+

```
</details>


<summary>How container communicate with each others & with host</summary>
<details>
   
### 1Ô∏è‚É£ Step 1: Creating Namespaces
```
ip netns add netns1
ip netns add netns2
```
Each netns = a virtual container with its own:

- Network interfaces (lo, etc.)
- Routing table
- ARP table
- iptables rules

They‚Äôre completely isolated from each other and from the host‚Äôs default namespace.

üì¶ Think of each as a container without Docker ‚Äî only networking isolation.

### 2Ô∏è‚É£ Step 2: Virtual Ethernet Pair (veth)
```
ip link add veth1 type veth peer name ceth1
ip link add veth2 type veth peer name ceth2
```

```
[Host:veth1] <====> [netns1:ceth1]
[Host:veth2] <====> [netns2:ceth2]
```

You later ‚Äúplug‚Äù one end into a namespace:
```
ip link set ceth1 netns netns1
ip link set ceth2 netns netns2
```

Now, ceth1 lives inside netns1, and ceth2 lives inside netns2.

### 3Ô∏è‚É£ Step 3: Assign IPs
#### On Host
````
ip addr add 172.16.16.201/24 dev veth1
ip addr add 172.16.17.201/24 dev veth2
ip link set veth1 up
ip link set veth2 up
````

#### Inside Containers
```
ip netns exec netns1 ip addr add 172.16.16.101/24 dev ceth1
ip netns exec netns2 ip addr add 172.16.17.101/24 dev ceth2
```

‚úÖ Each pair now forms a mini subnet:
```
netns1 <-> host (172.16.16.0/24)
netns2 <-> host (172.16.17.0/24)
```

### 4Ô∏è‚É£ Step 4: Default Route + Gateway
Each namespace needs a way to reach outside its subnet (via host):

```
ip netns exec netns1 ip route add default via 172.16.16.201 dev ceth1
ip netns exec netns2 ip route add default via 172.16.17.201 dev ceth2
```

Meaning:
- "To reach any IP not in 172.16.16.0/24, send to 172.16.16.201 (the host side)."

### 5Ô∏è‚É£ Step 5: Enable Host Routing + NAT
To forward traffic between these namespaces or to the Internet, the host must act as a router.
#### (a) Enable IP forwarding
```
sysctl -w net.ipv4.ip_forward=1
```

#### (b) Add NAT rules
```
gw_dev=$(ip -j route show | jq -r '.[]|select(.dst == "default") | .dev')
iptables -t nat -A POSTROUTING -s 172.16.16.0/24 -o ${gw_dev} -j MASQUERADE
iptables -t nat -A POSTROUTING -s 172.16.17.0/24 -o ${gw_dev} -j MASQUERADE
```
- This masquerades packets leaving the host to the Internet so replies return correctly.
- Without NAT, Internet servers would see source IPs like 172.16.16.101, which are private and unroutable.

### 6Ô∏è‚É£ Step 6: Verification
#### ‚úÖ netns1 ‚Üî host
```
ip netns exec netns1 ping 172.16.16.201
```

#### ‚úÖ netns2 ‚Üî host
```
ip netns exec netns2 ping 172.16.17.201
```

#### ‚úÖ netns1 ‚Üî netns2
- For this, you need the host to route between subnets 172.16.16.0/24 and 172.16.17.0/24.
- The host already has both interfaces up, so routing works automatically as long as IP forwarding is enabled.

### ‚úÖ netns ‚Üî Internet
```
ip netns exec netns1 ping 1.1.1.1
ip netns exec netns2 ping 1.1.1.1
```

### üåê Visualization
```
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ           Host               ‚îÇ
            ‚îÇ                              ‚îÇ
 Internet ‚áÑ eth0                           ‚îÇ
            ‚îÇ                              ‚îÇ
            ‚îÇ  172.16.16.201 ‚îÄ veth1 ‚îÄ‚îê
            ‚îÇ                         ‚îÇ
            ‚îÇ  172.16.17.201 ‚îÄ veth2 ‚îÄ‚îê
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ                         ‚îÇ
   netns1:ceth1               netns2:ceth2
   172.16.16.101              172.16.17.101
```
</details>

<summary>How to reach the outside world (e.g. the Internet) from inside the container?</summary>
<details>
 
</details>

## Union File System
<summary>What is it</summary>
<details>
 <img width="1219" height="596" alt="Screenshot 2025-11-11 at 21 45 05" src="https://github.com/user-attachments/assets/a3585973-5492-4a94-a7cc-1412b7c46d11" />
   
When Docker runs a container, the container‚Äôs filesystem is constructed from multiple layers: <br>
- Base Image layers (read-only) ‚Üí coming from image
- Container layer (read-write) ‚Üí created when container starts

A union mount (also called a union filesystem) is a technique to merge multiple directories (layers) into a single unified view. <br>
After merge:
```
/ (root)
  |-- bin/
  |-- etc/
  |-- usr/
  |-- (files from all layers combined)
```

### Why Do We Need Union Mount in Docker?
| Problem Without UnionFS                                                    | Solution With UnionFS                              |
| -------------------------------------------------------------------------- | -------------------------------------------------- |
| Every container would need a full copy of the OS filesystem ‚Üí large & slow | Layers are shared ‚Üí smaller images, faster startup |
| If each container modifies files, it overwrites core OS layers             | Container writes go only to **top writable layer** |
| Image updates would require full rebuilds                                  | Layers can be reused, cached, shared               |

Key Feature: Copy-on-Write <br>
```
Before Write:
  read file from RO layer

Write request ‚Üí
  copy file to RW layer
  modify in RW layer
```
<img width="1199" height="604" alt="Screenshot 2025-11-11 at 21 50 05" src="https://github.com/user-attachments/assets/d6faf730-2799-486f-bd46-ee09b3c6ef05" />

</details>
